## Clustering: an Introduction

The purpose of clstering data is to identify unique differences that cannot be otherwise determined by looking at data. The algorithms exploit different mathematical approaches to dfferentiate one data point form the other and conglomerate the points that has similar properties. It's an unsupervised learning method and is highly used in statistical data analysis. 

The most well known clustering method is the k-means clustering and we'll study here forth how to use the algorithm with our data

**K- Means Clustering**

The basic algorithms in step wise is as follows, 
  1. Determine the number of classes the data needs to be clsutered and randomly initialize the cluster centroid of each class within the data points. (Determining the number of clusters can be done by having a look in to the data with visualization techniques such as Prciple Component Analysis(PCA) or Selef Organizing Maps(Kohonen Networks) - we'll study those in upcoming lessons. Also it's valuable to remember that the algorithm is sensitive for the initial initialization of centroid points which we did randomly here)
  2. Each data point is assigned to the nearest neighbouring cluster centroid according to the distance from the centroid. 
  3. Based on assigned datapoints we compute the new centroid as the mean of the all vectors of the data points thus assigned. 
  4. Repeat the above steps for pre defined number of iterations or until the centroids don't change the poition much in the step 3. 
  
we'll implement K-means using several API and from scratch as well. To begin with we'll implement the k-means clustering offered by the scikit learn library. Firstly we'll generate our own dataset using scikit learn function and then move to a real world dataset. 

The dataset generated by the scikit learn _make blob_

The _make blob_ function outputs data points according to a isotropic gaussian distribution (find out isotropic gaussian meaning here : https://math.stackexchange.com/a/2137851)
```python
from sklearn.datasets import make_blobs
import matplotlib.pyplot as plt

X, y = make_blobs(n_samples=500,
                  n_features=2,
                  centers=4,
                  cluster_std=1,
                  center_box=(-10.0, 10.0),
                  shuffle=True,
                  random_state=1)

print(X)
print(y)

plt.scatter(X[:,0],X[:,1],marker = "x", s= 12)
plt.show()
```      
following is the graph generated above, 
<img src="Figure_1.png" alt="blobs" class="inline"/>

You can see that the dataset contains 4 clusters. We can implement the kmeans clustering directly on this data and obtain the separation of clusters. In reality the real world problems mostly doesn't come with how many clusters that it contains, usually it's one of the questions that needs to be answered. Hence there are several ways we can use to know how many clusters the dataset may contain. 

The scikit lean kmeans algorithms can be implemented as follows, 
The algorithm tries to minimise a parameter known as inertia or within-cluster sum-of-squares. The algorithm clusters the data in groups with equal variance. (more about within-cluster sum-of-squares can be found here https://discuss.analyticsvidhya.com/t/what-is-within-cluster-sum-of-squares-by-cluster-in-k-means/2706/2). 



